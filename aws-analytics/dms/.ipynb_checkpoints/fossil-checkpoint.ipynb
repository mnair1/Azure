{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# MODIFY WITH CARE\n",
    "# Standard libraries to be used in AWS Glue jobs\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, array, ArrayType, DateType\n",
    "from pyspark.sql import Row, Column\n",
    "import datetime\n",
    "import json\n",
    "import boto3\n",
    "import logging\n",
    "import calendar\n",
    "import uuid\n",
    "import time\n",
    "from dateutil import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_partition():\n",
    "    return str(datetime.datetime.now().date())\n",
    "\n",
    "def get_raw_table_name(tables, i):\n",
    "    location=tables['TableList'][i]['StorageDescriptor']['Location']\n",
    "    return location[location.rindex(\".\")+1: len(location) ]\n",
    "\n",
    "def get_raw_table_columns(client, catalog_database, catalog_table):\n",
    "    table = client.get_table(DatabaseName=catalog_database,Name=catalog_table)\n",
    "    return table['Table']['StorageDescriptor']['Columns']\n",
    "\n",
    "def handle_error(spark, log, message, region_name, log_bucket, job_log_dir, job_log_location, partition, sns_arn):\n",
    "    '''\n",
    "        function: handle_error\n",
    "        description: Stops the program, logs the event, sends a SNS notification.\n",
    "\n",
    "        Args:\n",
    "            spark: Spark session object.\n",
    "            log: List of logs.\n",
    "            message: Exception that has happened.\n",
    "            region_name: Name of the AWS region.\n",
    "            log_bucket: Bucket name for logs.\n",
    "            job_log_dir: Key of logs directory.\n",
    "            job_log_location: Location of logs.\n",
    "            partition: Date by which logs are paritioned.\n",
    "            sns_arn: ARN of SNS topic.\n",
    "    '''\n",
    "    append_log(log, str(datetime.datetime.now()), str(message), 'FAILURE')\n",
    "    log_df=create_log_df(spark, log)\n",
    "    log_df.show()\n",
    "    write_s3_file(log_df, job_log_location, '', partition, format='CSV')\n",
    "    job_notification_sns(region_name, log_bucket, job_log_dir, partition, sns_arn)\n",
    "    raise message\n",
    "\n",
    "def create_crawler(client, crawler_name, iam_role_arn, database_name):\n",
    "    return client.create_crawler(\n",
    "        Name=crawler_name,\n",
    "        Role=iam_role_arn,\n",
    "        DatabaseName=database_name,\n",
    "        Targets={\n",
    "            'S3Targets':[\n",
    "                {'Path':'s3://bucket/placeholder'}\n",
    "            ]}\n",
    "        )\n",
    "\n",
    "def read_table_from_catalog(glueContext, database, table_name):\n",
    "    '''\n",
    "    function: read_table_from_catalog\n",
    "    description: Reads a table using Glue catalog.\n",
    "\n",
    "    Args:\n",
    "        glueContext: GlueContext class object.\n",
    "        database: Glue catalog database name.\n",
    "        table_name: Glue catalog table name.\n",
    "    \n",
    "    Returns:\n",
    "        spark.sql.dataframe: Returns a object of Spark Dataframe containing data of the table coming from Glue catalog.\n",
    "    '''\n",
    "    return glueContext.create_dynamic_frame.from_catalog(\n",
    "             database=database,\n",
    "             table_name=table_name).toDF()\n",
    " \n",
    "def write_s3_file(df, table_location, table, partition=None, uid=None, format='PARQUET', delimiter='\\t', coalesce=1, header=False):\n",
    "    '''\n",
    "    function: write_s3_file\n",
    "    description: Write a file of either Parquet or CSV format to S3.\n",
    "\n",
    "    Args:\n",
    "        df: spark.sql.dataframe to be written to S3.\n",
    "        table_location: Location of where the file should be stored.\n",
    "        table: Name of the table.\n",
    "        partition: Date by which the file should be stored in S3.\n",
    "        format: Format in which the file should be stored. (PARQUET is default)\n",
    "        delimited: How to file should be delimited. (Applicable on CSV files only)\n",
    "        coalesce: Number of spark.sql.dataframe partitions.\n",
    "    '''\n",
    "    try:\n",
    "        if format == 'PARQUET':\n",
    "            if uid is None:\n",
    "                df.write.parquet(table_location+'/'+table+'/'+partition)\n",
    "            else:\n",
    "                df.write.parquet(table_location+'/'+table+'/'+uid+'/'+partition)\n",
    "        if format == 'CSV':\n",
    "            if partition is None:\n",
    "                df.coalesce(coalesce).write.option(\"delimiter\", delimiter).option(\"header\", \"true\").option(\"quoteAll\", \"true\").option(\"quote\", \"\\\"\").csv(table_location+ '/' + uid + '/' + table)\n",
    "            elif uid is None:\n",
    "                df.coalesce(coalesce).write.option(\"delimiter\", delimiter).option(\"quote\", \"\\\"\").option(\"quoteAll\", \"true\").csv(table_location +'/' + partition)\n",
    "            else:\n",
    "                df.coalesce(coalesce).write.option(\"delimiter\", delimiter).option(\"quote\", \"\\\"\").option(\"quoteAll\", \"true\").csv(table_location +'/' + uid + '/' + partition)\n",
    "    except Exception as e:\n",
    "        raise Exception(e)\n",
    "\n",
    "def compare_row_count(spark, glue, glueContext, source_database, destination_database):\n",
    "    source_tables = glue.get_tables(DatabaseName=source_database)\n",
    "    row_count_list = []\n",
    "    \n",
    "    for i in range(1,len(source_tables['TableList'])):\n",
    "    #for i in range(1,2):\n",
    "        source_table = source_tables['TableList'][i]['Name']  \n",
    "        destination_table = get_raw_table_name(source_tables, i)\n",
    "\n",
    "        source_df = read_table_from_catalog(glueContext, database=source_database, table_name=source_table)\n",
    "        source_row_count=source_df.count()\n",
    "        #print(source_table+'  '+destination_table+'  '+str(source_row_count))\n",
    "        try:\n",
    "            if (source_row_count > 0):\n",
    "                destination = glue.get_partitions(DatabaseName=destination_database, TableName=destination_table)\n",
    "                destination_df = spark.read.load(destination['Partitions'][0]['StorageDescriptor']['Location'])\n",
    "                \n",
    "                for partition in destination['Partitions'][1:]:\n",
    "                    desintation_location = partition['StorageDescriptor']['Location']\n",
    "                    destination_df = destination_df.union(spark.read.load(desintation_location))\n",
    "\n",
    "                destination_row_count=destination_df.count()\n",
    "              \n",
    "                if source_row_count == destination_row_count:\n",
    "                    matches = 'Rows count match'\n",
    "                else:\n",
    "                    matches = '**** Rows count mismatch ****'\n",
    "                    \n",
    "                row_count_list.append([source_table, source_row_count, destination_table, destination_row_count, matches])\n",
    "        except Exception as e:\n",
    "            print(e, source_table, destination_table)\n",
    "    row_count_df = spark.createDataFrame(row_count_list, schema=['Source', 'Source row count', 'Destination', 'Destination row count', 'Status'])\n",
    "    return row_count_df\n",
    "\n",
    "def append_path_to_list(list, location, table_name):\n",
    "    list.append({'Path': location + '/' + table_name})\n",
    "    \n",
    "def update_crawler(client, crawler_name, s3targets):\n",
    "    client.update_crawler(\n",
    "        Name=crawler_name,\n",
    "        Targets = {'S3Targets':s3targets}\n",
    "            \n",
    "    )\n",
    "    \n",
    "def start_crawler(client, crawler_name):\n",
    "    print(crawler_name + ' started.')\n",
    "    \n",
    "    # Getting PRE-RUN READY status.\n",
    "    while(True):\n",
    "        time.sleep(1)\n",
    "        response = client.get_crawler(\n",
    "                        Name=crawler_name\n",
    "                   )\n",
    "        \n",
    "        if response['Crawler']['State'] == 'READY':\n",
    "            print(response['Crawler']['State'])\n",
    "            break\n",
    "            \n",
    "    client.start_crawler(\n",
    "        Name=crawler_name\n",
    "    )\n",
    "    \n",
    "    # Getting RUNNING status for stdout.            \n",
    "    while(True):\n",
    "        time.sleep(15)\n",
    "        response = client.get_crawler(\n",
    "                        Name=crawler_name\n",
    "                   )\n",
    "        \n",
    "        if response['Crawler']['State'] == 'RUNNING':\n",
    "            print(response['Crawler']['State'])\n",
    "            break\n",
    "        \n",
    "    # Getting STOPPING status for stdout.\n",
    "    while(True):\n",
    "        time.sleep(1)\n",
    "        response = client.get_crawler(\n",
    "                        Name=crawler_name\n",
    "                   )\n",
    "        \n",
    "        if response['Crawler']['State'] == 'STOPPING':\n",
    "            print(response['Crawler']['State'])\n",
    "            break\n",
    "    \n",
    "   # Getting READY status.\n",
    "    while(True):\n",
    "        time.sleep(1)\n",
    "        response = client.get_crawler(\n",
    "                        Name=crawler_name\n",
    "                   )\n",
    "        \n",
    "        if response['Crawler']['State'] == 'READY':\n",
    "            print(response['Crawler']['State'])\n",
    "            break\n",
    "\n",
    "def delete_crawler(client, crawler_name):\n",
    "    # Getting READY status before deleting making sure it won't delete a running crawler.\n",
    "    while(True):\n",
    "        time.sleep(1)\n",
    "        response = client.get_crawler(\n",
    "                        Name=crawler_name\n",
    "                   )\n",
    "        \n",
    "        if response['Crawler']['State'] == 'READY':\n",
    "            print(response['Crawler']['State'])\n",
    "            break\n",
    "    \n",
    "    client.delete_crawler(\n",
    "        Name=crawler_name\n",
    "    )\n",
    "    \n",
    "    print(crawler_name + ' deleted.')\n",
    "\n",
    "def append_log(log, logdate, id, message):\n",
    "    '''\n",
    "    function: append_log\n",
    "    description: Appends to the log.\n",
    "\n",
    "    Args:\n",
    "        log: spark.sql.dataframe of logs.\n",
    "        logdate: Date on which the log was created.\n",
    "        id: Id to identify the logs.\n",
    "        message: Message of the log.\n",
    "\n",
    "    returns:\n",
    "        spark.sql.dataframe: Returns a dataframe containing newly appended log.\n",
    "    '''\n",
    "    return log.append(Row(logdate,id, message))\n",
    "\n",
    "def delete_catalog_table(client, database, table):\n",
    "    try:\n",
    "        response = client.delete_table(DatabaseName=database,Name=table)\n",
    "    except Exception as e:\n",
    "        print(table+' does not exist in glue catalog')\n",
    "\n",
    "def job_notification_sns(region_name, log_bucket, job_log_dir, partition, sns_arn):\n",
    "    '''\n",
    "    function: format_log_date\n",
    "    description: Returns current timestamp (date and time) for logs in Amazon Athena format.\n",
    "\n",
    "    Args:\n",
    "        region_name: AWS Region in which the SNS and S3.\n",
    "        log_bucket: Name of the log bucket.\n",
    "        job_log_dir: Folder in which the logs will be placed.\n",
    "        partition: Folder (named as date of log creation) by which the logs will be partitioned.\n",
    "        sns_arn: ARN of SNS topic to which the notificaions is to be published.\n",
    "    '''\n",
    "\n",
    "    sns = boto3.client('sns', region_name=region_name)\n",
    "    s3 = boto3.client('s3' , region_name=region_name)\n",
    "    s3r = boto3.resource('s3' , region_name=region_name)\n",
    "\n",
    "    response = s3.list_objects(Bucket = log_bucket, Prefix = job_log_dir+'/'+partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "#   Turn DEBUG ON if temporary tables are desired. DEBUG creates additional temporary tables for every step in the script.\n",
    "#    DEBUG: If 'True', the script will store all dataframes, including the temporary ones.\n",
    "#    INTERACTIVE: Set 'True' for Jupyter Notebook.\n",
    "#                 Set 'False' for AWS Glue ETL job.\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "INTERACTIVE=True\n",
    "DEBUG=True  \n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# MODIFY AS PER JOB\n",
    "#    These are the variables that drive the whole job.\n",
    "#    Modify each variable for each job accordingly.\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Variables for GLue Tables\n",
    "JOB_NAME='ingest_raw_fossil'\n",
    "REGION_NAME='us-east-1'\n",
    "UID=uuid.uuid4().hex\n",
    "PARTITION=\"dt=\"+get_partition()\n",
    "\n",
    "# Variables for Job Log\n",
    "LOG_BUCKET='ds-operations-1111-raw'\n",
    "JOB_LOG_DIR='log/'+JOB_NAME\n",
    "JOB_LOG_LOCATION='s3://'+LOG_BUCKET+'/'+JOB_LOG_DIR\n",
    "SNS_ARN='arn:aws:sns:us-east-1:175908995626:monitor-alert'\n",
    "\n",
    "# Variables for RAW Table Location on S3 and Glue Catalog Database\n",
    "CATALOG_DATABASE='fossil'\n",
    "RAW_DATABASE='raw_fossil'\n",
    "RAW_BUCKET='ds-operations-1111-raw'\n",
    "RAW_TAB_LOCATION='s3://'+RAW_BUCKET+'/'+CATALOG_DATABASE+'/'+UID\n",
    "\n",
    "\n",
    "# Variables for crawler.\n",
    "CRAWLER_NAME=JOB_NAME+'_'+UID\n",
    "CRAWLER_ARN='arn:aws:iam::175908995626:role/glue-role'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# MODIFY WITH CARE\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "\n",
    "if not INTERACTIVE:\n",
    "    spark = glueContext.spark_session\n",
    "    \n",
    "client = boto3.client('glue', region_name=REGION_NAME)\n",
    "log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Path': 's3://ds-operations-1111-raw/fossil/d72ac211a70b43c58e9da4fa46e74f65/coal_prod'}, {'Path': 's3://ds-operations-1111-raw/fossil/d72ac211a70b43c58e9da4fa46e74f65/fossil_capita'}, {'Path': 's3://ds-operations-1111-raw/fossil/d72ac211a70b43c58e9da4fa46e74f65/gas_prod'}, {'Path': 's3://ds-operations-1111-raw/fossil/d72ac211a70b43c58e9da4fa46e74f65/oil_prod'}]"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Script to fetch data from the EDW RDS instance and save results to S3 \n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "try:   \n",
    "    first = True\n",
    "    i=0\n",
    "    s3pathlist=[]\n",
    "    while True:\n",
    "        if first:\n",
    "            tables = client.get_tables(DatabaseName=CATALOG_DATABASE)\n",
    "            first = False\n",
    "        else:\n",
    "            tables = client.get_tables(DatabaseName=CATALOG_DATABASE, NextToken=tables['NextToken'])\n",
    "        \n",
    "        for i in range(0, len(tables['TableList'])):\n",
    "            raw_table=get_raw_table_name(tables, i)\n",
    "            catalog_table=tables['TableList'][i]['Name']\n",
    "            columns=get_raw_table_columns(client, CATALOG_DATABASE, catalog_table)\n",
    "            table_df = read_table_from_catalog(glueContext, database=CATALOG_DATABASE, table_name=catalog_table)\n",
    "            columns_df=table_df.columns\n",
    "            row_count=table_df.count()\n",
    "            #print(catalog_table+'  '+str(row_count))\n",
    "        \n",
    "            for column in columns_df:\n",
    "                table_df=table_df.withColumnRenamed(column, column.replace(' ', '_'))\n",
    " \n",
    "            if (row_count > 0):\n",
    "                write_s3_file(table_df, RAW_TAB_LOCATION, raw_table, PARTITION)\n",
    "                append_path_to_list(s3pathlist, RAW_TAB_LOCATION, raw_table)  \n",
    "                append_log(log, str(datetime.datetime.now()), raw_table+' - Row Count: ', row_count)\n",
    "            else:\n",
    "                append_log(log, str(datetime.datetime.now()), raw_table+' - Table Ignored: ','0 rows')\n",
    "    \n",
    "    \n",
    "        if 'NextToken' not in tables.keys():\n",
    "            break\n",
    "    print(s3pathlist)\n",
    "        \n",
    "except Exception as e:\n",
    "    if INTERACTIVE:\n",
    "        raise e\n",
    "    else:\n",
    "        handle_error(spark, log, e, REGION_NAME, LOG_BUCKET, JOB_LOG_DIR+'/'+UID, JOB_LOG_LOCATION, PARTITION, SNS_ARN)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coal_prod does not exist in glue catalog\n",
      "fossil_capita does not exist in glue catalog\n",
      "gas_prod does not exist in glue catalog\n",
      "oil_prod does not exist in glue catalog"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Script to delete previously catalogued tables \n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "try:\n",
    "    first = True\n",
    "    while True:\n",
    "        if first:\n",
    "            tables = client.get_tables(DatabaseName=CATALOG_DATABASE)\n",
    "            first = False\n",
    "        else:\n",
    "            tables = client.get_tables(DatabaseName=CATALOG_DATABASE, NextToken=tables['NextToken'])\n",
    "        \n",
    "        for i in range(0, len(tables['TableList'])):\n",
    "            raw_table=get_raw_table_name(tables, i)\n",
    "            delete_catalog_table(client, RAW_DATABASE, raw_table)\n",
    "    \n",
    "        if 'NextToken' not in tables.keys():\n",
    "            break\n",
    "except Exception as e:\n",
    "    if INTERACTIVE:\n",
    "        raise e\n",
    "    else:\n",
    "        handle_error(spark, log, e, REGION_NAME, LOG_BUCKET, JOB_LOG_DIR+'/'+UID, JOB_LOG_LOCATION, PARTITION, SNS_ARN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '511a17b7-e47d-11e9-bf61-29ce2eb39ff3', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Tue, 01 Oct 2019 18:57:29 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2', 'connection': 'keep-alive', 'x-amzn-requestid': '511a17b7-e47d-11e9-bf61-29ce2eb39ff3'}, 'RetryAttempts': 0}}\n",
      "ingest_raw_fossil_d72ac211a70b43c58e9da4fa46e74f65 started.\n",
      "READY\n",
      "RUNNING\n",
      "STOPPING\n",
      "READY\n",
      "READY\n",
      "ingest_raw_fossil_d72ac211a70b43c58e9da4fa46e74f65 deleted."
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Crawl tables\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "try:\n",
    "    create_crawler(client, CRAWLER_NAME, CRAWLER_ARN, RAW_DATABASE)\n",
    "    update_crawler(client, CRAWLER_NAME, s3pathlist)\n",
    "    start_crawler(client, CRAWLER_NAME)\n",
    "    delete_crawler(client, CRAWLER_NAME)\n",
    "    append_log(log, str(datetime.datetime.now()), 'Crawling of AMS tables : ','SUCCESS')\n",
    "except Exception as e:\n",
    "    if INTERACTIVE:\n",
    "        delete_crawler(client, CRAWLER_NAME)\n",
    "        raise e\n",
    "    else:\n",
    "        delete_crawler(client, CRAWLER_NAME)\n",
    "        handle_error(spark, log, e, REGION_NAME, LOG_BUCKET, JOB_LOG_DIR+'/'+UID, JOB_LOG_LOCATION, PARTITION, SNS_ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Perform count comparisons\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "compare_df=compare_row_count(spark, client, glueContext, CATALOG_DATABASE, RAW_DATABASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# save logs and send logs to SNS topic for distribution\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "write_s3_file(compare_df, JOB_LOG_LOCATION, PARTITION, coalesce=1, delimiter=',',format='CSV', header=True, uid=UID)\n",
    "job_notification_sns(REGION_NAME, LOG_BUCKET, JOB_LOG_DIR+'/'+UID, PARTITION, SNS_ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
