{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# MODIFY WITH CARE\n",
    "# Standard libraries to be used in AWS Glue jobs\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, array, ArrayType, DateType\n",
    "from pyspark.sql import Row, Column\n",
    "import datetime\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "import logging\n",
    "import calendar\n",
    "import uuid\n",
    "from dateutil import relativedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from awsglue.utils import getResolvedOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit Variables below as per AWS account\n",
    "\n",
    "BATCH_MODE=False\n",
    "\n",
    "if BATCH_MODE:\n",
    "    args = getResolvedOptions(sys.argv, ['JOB_NAME','ARG_JOB_DATE'])\n",
    "    JOB_DATE=args['ARG_JOB_DATE']\n",
    "else:\n",
    "    JOB_DATE='20200708' \n",
    "\n",
    "S3PATHREAD=\"s3://data-bucket-1234/data/cricket-world-cup/\"+JOB_DATE+\"/\"\n",
    "S3PATHWRITE=\"s3://data-bucket-1234/curated/cricket-world-cup\"\n",
    "\n",
    "PARTITION='dt='+JOB_DATE\n",
    "CATALOG_TABLE_LIST=[]\n",
    "\n",
    "CURATED_DATABASE='curated_worldcup'\n",
    "\n",
    "BOWLER_TBL='bowlers'\n",
    "\n",
    "CRAWLER_NAME='worldcup'\n",
    "CRAWLER_ARN='arn:aws:iam::175908995626:role/glue-role'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "client = boto3.client('glue', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_s3_file(spark, type, path, delimiter='|', header='true'):\n",
    "    if (type == 'CSV'):\n",
    "        return spark.read.format(\"com.databricks.spark.csv\").option(\"header\", header).option(\"delimiter\", delimiter).load(path)\n",
    "    if (type == 'PARQUET'):\n",
    "        return spark.read.parquet(path)\n",
    "    \n",
    "def write_s3_file(df, table_location, table, partition=None, format='PARQUET', delimiter='\\t', coalesce=1, header=False):\n",
    "    if format == 'PARQUET':\n",
    "        df.write.parquet(table_location+'/'+table+'/'+partition, mode = \"overwrite\")\n",
    "    if format == 'CSV':\n",
    "        df.coalesce(coalesce).write.option(\"delimiter\", delimiter).option(\"quote\", \"\\\"\").option(\"quoteAll\", \"true\").csv(table_location +'/' + partition)\n",
    "        \n",
    "def append_path_to_list(list, location, table_name):\n",
    "    list.append({'Path': location + '/' + table_name})\n",
    "    \n",
    "def delete_catalog_table(client, database, table):\n",
    "    try:\n",
    "        response = client.delete_table(DatabaseName=database,Name=table)\n",
    "    except Exception as e:\n",
    "        print(table+' does not exist in glue catalog')\n",
    "        \n",
    "def create_crawler(client, crawler_name, iam_role_arn, database_name):\n",
    "    return client.create_crawler(\n",
    "        Name=crawler_name,\n",
    "        Role=iam_role_arn,\n",
    "        DatabaseName=database_name,\n",
    "        Targets={\n",
    "            'S3Targets':[\n",
    "                {'Path':'s3://bucket/placeholder'}\n",
    "            ]}\n",
    "        )\n",
    "    \n",
    "def update_crawler(client, crawler_name, s3targets):\n",
    "    client.update_crawler(\n",
    "        Name=crawler_name,\n",
    "        Targets = {'S3Targets':s3targets}\n",
    "            \n",
    "    )\n",
    "    \n",
    "def start_crawler(client, crawler_name):\n",
    "    print(crawler_name + ' started.')\n",
    "    \n",
    "    # Getting PRE-RUN READY status.\n",
    "    while(True):\n",
    "        time.sleep(1)\n",
    "        response = client.get_crawler(\n",
    "                        Name=crawler_name\n",
    "                   )\n",
    "        \n",
    "        if response['Crawler']['State'] == 'READY':\n",
    "            print(response['Crawler']['State'])\n",
    "            break\n",
    "            \n",
    "    client.start_crawler(\n",
    "        Name=crawler_name\n",
    "    )\n",
    "    \n",
    "    # Getting RUNNING status for stdout.            \n",
    "    while(True):\n",
    "        time.sleep(15)\n",
    "        response = client.get_crawler(\n",
    "                        Name=crawler_name\n",
    "                   )\n",
    "        \n",
    "        if response['Crawler']['State'] == 'RUNNING':\n",
    "            print(response['Crawler']['State'])\n",
    "            break\n",
    "        \n",
    "    # Getting STOPPING status for stdout.\n",
    "    while(True):\n",
    "        time.sleep(1)\n",
    "        response = client.get_crawler(\n",
    "                        Name=crawler_name\n",
    "                   )\n",
    "        \n",
    "        if response['Crawler']['State'] == 'STOPPING':\n",
    "            print(response['Crawler']['State'])\n",
    "            break\n",
    "    \n",
    "   # Getting READY status.\n",
    "    while(True):\n",
    "        time.sleep(1)\n",
    "        response = client.get_crawler(\n",
    "                        Name=crawler_name\n",
    "                   )\n",
    "        \n",
    "        if response['Crawler']['State'] == 'READY':\n",
    "            print(response['Crawler']['State'])\n",
    "            break\n",
    "\n",
    "def delete_crawler(client, crawler_name):\n",
    "    # Getting READY status before deleting making sure it won't delete a running crawler.\n",
    "    while(True):\n",
    "        time.sleep(1)\n",
    "        response = client.get_crawler(\n",
    "                        Name=crawler_name\n",
    "                   )\n",
    "        \n",
    "        if response['Crawler']['State'] == 'READY':\n",
    "            print(response['Crawler']['State'])\n",
    "            break\n",
    "    \n",
    "    client.delete_crawler(\n",
    "        Name=crawler_name\n",
    "    )\n",
    "    \n",
    "    print(crawler_name + ' deleted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowler_df=read_s3_file(spark, 'CSV', S3PATHREAD+\"Bowler_data.csv\", delimiter=',',header='true')\n",
    "bowler_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowler_df=bowler_df \\\n",
    "          .withColumnRenamed('_c0','ID') \\\n",
    "          .withColumn('Overs', f.col('Overs').cast(IntegerType())) \\\n",
    "          .withColumn('Mdns', f.col('Mdns').cast(IntegerType())) \\\n",
    "          .withColumn('Runs', f.col('Runs').cast(IntegerType())) \\\n",
    "          .withColumn('Wkts', f.col('Wkts').cast(IntegerType())) \\\n",
    "          .withColumn('Econ', f.col('Econ').cast(IntegerType())) \\\n",
    "          .withColumn('Ave', f.col('Ave').cast(IntegerType())) \\\n",
    "          .withColumn('Player_ID', f.col('Player_ID').cast(IntegerType()))\n",
    "\n",
    "bowler_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowler_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowler_df.registerTempTable('bowlers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowler_df = spark.sql(\" SELECT `ID`,`Overs`,`Mdns`,`Runs`,`Wkts`,`Econ`,`Ave`,`SR`, \" \\\n",
    "                      \" SUBSTR(`Opposition`, 2,20) as `Opposition`, \" \\\n",
    "                      \" `Ground`, TO_DATE(`Start Date`, 'dd MMM yyyy') AS Start_Date, \" \\\n",
    "                      \" SUBSTR(Match_ID, 6,10) AS Match_Number,`Bowler`,`Player_ID` \" \\\n",
    "                      \" FROM bowlers\")\n",
    "bowler_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_s3_file(bowler_df, S3PATHWRITE, BOWLER_TBL, PARTITION)\n",
    "append_path_to_list(CATALOG_TABLE_LIST, S3PATHWRITE, BOWLER_TBL) \n",
    "print(CATALOG_TABLE_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering using Dataframe commands\n",
    "bowler_df.filter(bowler_df.Player_ID == 49619).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering using Spark SQL commands\n",
    "sqlContext.sql(\"SELECT * FROM bowlers where Player_ID = 49619\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average overs did a bowler bowl\n",
    "avg_bowler_df = bowler_df.groupBy(bowler_df.Player_ID).avg('Overs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_bowler_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_bowler_df = spark.sql(\"SELECT Player_ID, avg(Overs) FROM bowlers GROUP BY Player_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_bowler_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bowlers with lowest Economy Rate per over\n",
    "econ_bowler_df = spark.sql(\"SELECT Player_ID, (SELECT min(Econ) FROM bowlers) as Econ \" \\\n",
    "                            \" FROM bowlers WHERE Econ=(SELECT min(Econ) FROM bowlers)\")\n",
    "econ_bowler_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match Numbers played in Port of Spain\n",
    "econ_bowler_df = spark.sql(\"SELECT DISTINCT SUBSTR(Match_ID, 6,10) AS Match_Number \" \\\n",
    "                            \" FROM bowlers WHERE Ground='Port of Spain'\")\n",
    "econ_bowler_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Script to delete previously catalogued tables\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "delete_catalog_table(client, CURATED_DATABASE, BOWLER_TBL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Crawl tables\n",
    "# -------------------------------------------------------------------------\n",
    "create_crawler(client, CRAWLER_NAME, CRAWLER_ARN, CURATED_DATABASE)\n",
    "update_crawler(client, CRAWLER_NAME, CATALOG_TABLE_LIST)\n",
    "start_crawler(client,  CRAWLER_NAME)\n",
    "delete_crawler(client, CRAWLER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
