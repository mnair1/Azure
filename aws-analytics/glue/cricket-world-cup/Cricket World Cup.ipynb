{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>16</td><td>application_1563280589119_0017</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-32-108-120.ec2.internal:20888/proxy/application_1563280589119_0017/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-32-113-32.ec2.internal:8042/node/containerlogs/container_1563280589119_0017_01_000001/livy\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# MODIFY WITH CARE\n",
    "# Standard libraries to be used in AWS Glue jobs\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, array, ArrayType, DateType\n",
    "from pyspark.sql import Row, Column\n",
    "import datetime\n",
    "import json\n",
    "import boto3\n",
    "import time\n",
    "import logging\n",
    "import calendar\n",
    "import uuid\n",
    "from dateutil import relativedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from awsglue.utils import getResolvedOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_MODE=False\n",
    "\n",
    "if BATCH_MODE:\n",
    "    args = getResolvedOptions(sys.argv, ['JOB_NAME','ARG_JOB_DATE'])\n",
    "    JOB_DATE=args['ARG_JOB_DATE']\n",
    "else:\n",
    "    JOB_DATE='20190708' \n",
    "\n",
    "S3PATHREAD=\"s3://ds-operations-111-raw/worldcup/\"+JOB_DATE+\"/\"\n",
    "S3PATHWRITE=\"s3://ds-operations-111-curated\"\n",
    "\n",
    "PARTITION='dt='+JOB_DATE\n",
    "CATALOG_TABLE_LIST=[]\n",
    "\n",
    "CURATED_DATABASE='curated_worldcup'\n",
    "\n",
    "BOWLER_TBL='bowlers'\n",
    "\n",
    "CRAWLER_NAME='worldcup'\n",
    "CRAWLER_ARN='arn:aws:iam::956630041263:role/glue-role'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "client = boto3.client('glue', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_s3_file(spark, type, path, delimiter='|', header='true'):\n",
    "    if (type == 'CSV'):\n",
    "        return spark.read.format(\"com.databricks.spark.csv\").option(\"header\", header).option(\"delimiter\", delimiter).load(path)\n",
    "    if (type == 'PARQUET'):\n",
    "        return spark.read.parquet(path)\n",
    "    \n",
    "def write_s3_file(df, table_location, table, partition=None, format='PARQUET', delimiter='\\t', coalesce=1, header=False):\n",
    "    if format == 'PARQUET':\n",
    "        df.write.parquet(table_location+'/'+table+'/'+partition, mode = \"overwrite\")\n",
    "    if format == 'CSV':\n",
    "        df.coalesce(coalesce).write.option(\"delimiter\", delimiter).option(\"quote\", \"\\\"\").option(\"quoteAll\", \"true\").csv(table_location +'/' + partition)\n",
    "        \n",
    "def append_path_to_list(list, location, table_name):\n",
    "    list.append({'Path': location + '/' + table_name})\n",
    "    \n",
    "def delete_catalog_table(client, database, table):\n",
    "    try:\n",
    "        response = client.delete_table(DatabaseName=database,Name=table)\n",
    "    except Exception as e:\n",
    "        print(table+' does not exist in glue catalog')\n",
    "        \n",
    "def create_crawler(client, crawler_name, iam_role_arn, database_name):\n",
    "    return client.create_crawler(\n",
    "        Name=crawler_name,\n",
    "        Role=iam_role_arn,\n",
    "        DatabaseName=database_name,\n",
    "        Targets={\n",
    "            'S3Targets':[\n",
    "                {'Path':'s3://bucket/placeholder'}\n",
    "            ]}\n",
    "        )\n",
    "    \n",
    "def update_crawler(client, crawler_name, s3targets):\n",
    "    client.update_crawler(\n",
    "        Name=crawler_name,\n",
    "        Targets = {'S3Targets':s3targets}\n",
    "            \n",
    "    )\n",
    "    \n",
    "def start_crawler(client, crawler_name):\n",
    "    print(crawler_name + ' started.')\n",
    "    \n",
    "    # Getting PRE-RUN READY status.\n",
    "    while(True):\n",
    "        time.sleep(1)\n",
    "        response = client.get_crawler(\n",
    "                        Name=crawler_name\n",
    "                   )\n",
    "        \n",
    "        if response['Crawler']['State'] == 'READY':\n",
    "            print(response['Crawler']['State'])\n",
    "            break\n",
    "            \n",
    "    client.start_crawler(\n",
    "        Name=crawler_name\n",
    "    )\n",
    "    \n",
    "    # Getting RUNNING status for stdout.            \n",
    "    while(True):\n",
    "        time.sleep(15)\n",
    "        response = client.get_crawler(\n",
    "                        Name=crawler_name\n",
    "                   )\n",
    "        \n",
    "        if response['Crawler']['State'] == 'RUNNING':\n",
    "            print(response['Crawler']['State'])\n",
    "            break\n",
    "        \n",
    "    # Getting STOPPING status for stdout.\n",
    "    while(True):\n",
    "        time.sleep(1)\n",
    "        response = client.get_crawler(\n",
    "                        Name=crawler_name\n",
    "                   )\n",
    "        \n",
    "        if response['Crawler']['State'] == 'STOPPING':\n",
    "            print(response['Crawler']['State'])\n",
    "            break\n",
    "    \n",
    "   # Getting READY status.\n",
    "    while(True):\n",
    "        time.sleep(1)\n",
    "        response = client.get_crawler(\n",
    "                        Name=crawler_name\n",
    "                   )\n",
    "        \n",
    "        if response['Crawler']['State'] == 'READY':\n",
    "            print(response['Crawler']['State'])\n",
    "            break\n",
    "\n",
    "def delete_crawler(client, crawler_name):\n",
    "    # Getting READY status before deleting making sure it won't delete a running crawler.\n",
    "    while(True):\n",
    "        time.sleep(1)\n",
    "        response = client.get_crawler(\n",
    "                        Name=crawler_name\n",
    "                   )\n",
    "        \n",
    "        if response['Crawler']['State'] == 'READY':\n",
    "            print(response['Crawler']['State'])\n",
    "            break\n",
    "    \n",
    "    client.delete_crawler(\n",
    "        Name=crawler_name\n",
    "    )\n",
    "    \n",
    "    print(crawler_name + ' deleted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowler_df=read_s3_file(spark, 'CSV', S3PATHREAD+\"Bowler_data.csv\", delimiter=',',header='true')\n",
    "#bowler_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowler_df=bowler_df \\\n",
    "          .withColumnRenamed('_c0','ID') \\\n",
    "          .withColumn('Overs', f.col('Overs').cast(IntegerType())) \\\n",
    "          .withColumn('Mdns', f.col('Mdns').cast(IntegerType())) \\\n",
    "          .withColumn('Runs', f.col('Runs').cast(IntegerType())) \\\n",
    "          .withColumn('Wkts', f.col('Wkts').cast(IntegerType())) \\\n",
    "          .withColumn('Econ', f.col('Econ').cast(IntegerType())) \\\n",
    "          .withColumn('Ave', f.col('Ave').cast(IntegerType())) \\\n",
    "          .withColumn('Player_ID', f.col('Player_ID').cast(IntegerType()))\n",
    "\n",
    "#bowler_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Overs: integer (nullable = true)\n",
      " |-- Mdns: integer (nullable = true)\n",
      " |-- Runs: integer (nullable = true)\n",
      " |-- Wkts: integer (nullable = true)\n",
      " |-- Econ: integer (nullable = true)\n",
      " |-- Ave: integer (nullable = true)\n",
      " |-- SR: string (nullable = true)\n",
      " |-- Opposition: string (nullable = true)\n",
      " |-- Ground: string (nullable = true)\n",
      " |-- Start Date: string (nullable = true)\n",
      " |-- Match_ID: string (nullable = true)\n",
      " |-- Bowler: string (nullable = true)\n",
      " |-- Player_ID: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "bowler_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowler_df.registerTempTable('bowlers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5118"
     ]
    }
   ],
   "source": [
    "# Write Results to \n",
    "bowler_df = spark.sql(\" SELECT `ID`,`Overs`,`Mdns`,`Runs`,`Wkts`,`Econ`,`Ave`,`SR`, \" \\\n",
    "                      \" SUBSTR(`Opposition`, 2,20) as `Opposition`, \" \\\n",
    "                      \" `Ground`, TO_DATE(`Start Date`, 'dd MMM yyyy') AS Start_Date, \" \\\n",
    "                      \" SUBSTR(Match_ID, 6,10) AS Match_Number,`Bowler`,`Player_ID` \" \\\n",
    "                      \" FROM bowlers\")\n",
    "bowler_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Path': 's3://ds-operations-111-curated/bowlers'}]"
     ]
    }
   ],
   "source": [
    "write_s3_file(bowler_df, S3PATHWRITE, BOWLER_TBL, PARTITION)\n",
    "append_path_to_list(CATALOG_TABLE_LIST, S3PATHWRITE, BOWLER_TBL) \n",
    "print(CATALOG_TABLE_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    }
   ],
   "source": [
    "# Filtering using Dataframe commands\n",
    "bowler_df.filter(bowler_df.Player_ID == 49619).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    }
   ],
   "source": [
    "# Filtering using Spark SQL commands\n",
    "sqlContext.sql(\"SELECT * FROM bowlers where Player_ID = 49619\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average overs did a bowler bowl\n",
    "avg_bowler_df = bowler_df.groupBy(bowler_df.Player_ID).avg('Overs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|Player_ID|        avg(Overs)|\n",
      "+---------+------------------+\n",
      "|   326434|              null|\n",
      "|   311592| 8.253333333333334|\n",
      "|   325012| 6.225806451612903|\n",
      "|   297433|              null|\n",
      "|   379504| 8.590909090909092|\n",
      "|    25913| 8.158878504672897|\n",
      "|    19264|             7.875|\n",
      "|   793463|  8.62962962962963|\n",
      "|   318339|           5.34375|\n",
      "|   351588|             8.525|\n",
      "|   272279|              9.48|\n",
      "|   267192| 4.805555555555555|\n",
      "|     8917| 8.101123595505618|\n",
      "|   330902| 8.355555555555556|\n",
      "|   272477|               9.0|\n",
      "|    34102|2.5789473684210527|\n",
      "|     5334|2.3529411764705883|\n",
      "|   419873|              null|\n",
      "|   440970|               1.5|\n",
      "|   261354| 8.555555555555555|\n",
      "+---------+------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "avg_bowler_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_bowler_df = spark.sql(\"SELECT Player_ID, avg(Overs) FROM bowlers GROUP BY Player_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|Player_ID|        avg(Overs)|\n",
      "+---------+------------------+\n",
      "|   326434|              null|\n",
      "|   311592| 8.253333333333334|\n",
      "|   325012| 6.225806451612903|\n",
      "|   297433|              null|\n",
      "|   379504| 8.590909090909092|\n",
      "|    25913| 8.158878504672897|\n",
      "|    19264|             7.875|\n",
      "|   793463|  8.62962962962963|\n",
      "|   318339|           5.34375|\n",
      "|   351588|             8.525|\n",
      "|   272279|              9.48|\n",
      "|   267192| 4.805555555555555|\n",
      "|     8917| 8.101123595505618|\n",
      "|   330902| 8.355555555555556|\n",
      "|   272477|               9.0|\n",
      "|    34102|2.5789473684210527|\n",
      "|     5334|2.3529411764705883|\n",
      "|   419873|              null|\n",
      "|   440970|               1.5|\n",
      "|   261354| 8.555555555555555|\n",
      "+---------+------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "avg_bowler_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|Player_ID|Econ|\n",
      "+---------+----+\n",
      "|   311592|   0|\n",
      "|   320652|   0|\n",
      "+---------+----+"
     ]
    }
   ],
   "source": [
    "# Bowlers with lowest Economy Rate per over\n",
    "econ_bowler_df = spark.sql(\"SELECT Player_ID, (SELECT min(Econ) FROM bowlers) as Econ \" \\\n",
    "                            \" FROM bowlers WHERE Econ=(SELECT min(Econ) FROM bowlers)\")\n",
    "econ_bowler_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Match_Number|\n",
      "+------------+\n",
      "|        2546|\n",
      "|        3387|\n",
      "|        3895|\n",
      "|        3159|\n",
      "|        2382|\n",
      "|        2550|\n",
      "|        3160|\n",
      "|        3388|\n",
      "|        2542|\n",
      "|        2538|\n",
      "|        3383|\n",
      "|        2554|\n",
      "|        3896|\n",
      "|        2381|\n",
      "+------------+"
     ]
    }
   ],
   "source": [
    "# Match Numbers played in Port of Spain\n",
    "econ_bowler_df = spark.sql(\"SELECT DISTINCT SUBSTR(Match_ID, 6,10) AS Match_Number \" \\\n",
    "                            \" FROM bowlers WHERE Ground='Port of Spain'\")\n",
    "econ_bowler_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bowlers does not exist in glue catalog"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Script to delete previously catalogued tables\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "delete_catalog_table(client, CURATED_DATABASE, BOWLER_TBL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worldcup started.\n",
      "READY\n",
      "RUNNING\n",
      "STOPPING\n",
      "READY\n",
      "READY\n",
      "worldcup deleted."
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Crawl tables\n",
    "# -------------------------------------------------------------------------\n",
    "create_crawler(client, CRAWLER_NAME, CRAWLER_ARN, CURATED_DATABASE)\n",
    "update_crawler(client, CRAWLER_NAME, CATALOG_TABLE_LIST)\n",
    "start_crawler(client,  CRAWLER_NAME)\n",
    "delete_crawler(client, CRAWLER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
